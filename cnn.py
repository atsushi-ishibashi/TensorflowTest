# -*- coding: utf-8 -*-

import wave_data
import tensorflow as tf
import csv

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

#padding="SAME" : input size equal to output size
def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

def conv_neural_network():
    mnist = wave_data.read_data_sets()
    sess = tf.InteractiveSession()
    x = tf.placeholder("float", shape=[None, 1024])
    y_ = tf.placeholder("float", shape=[None, 2])
    keep_prob = tf.placeholder("float")

    x_image = tf.reshape(x, [-1,32,32,1])
    W_conv1 = weight_variable([3, 3, 1, 32])
    b_conv1 = bias_variable([32])
    W1_hist = tf.histogram_summary("conv1_weights",W_conv1)
    b1_hist = tf.histogram_summary("conv1_biases",b_conv1)
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    conv1_ce = -tf.reduce_sum(x_image*tf.log(tf.clip_by_value(h_conv1,1e-10,100.0)))
    conv1_ts = tf.train.AdamOptimizer(1e-4).minimize(conv1_ce)

    x_2 = tf.placeholder("float", shape=[None, 32, 32, 32])
    W_conv2 = weight_variable([3, 3, 32, 32])
    b_conv2 = bias_variable([32])
    W2_hist = tf.histogram_summary("conv2_weights",W_conv2)
    b2_hist = tf.histogram_summary("conv2_biases",b_conv2)
    h_conv2 = tf.nn.relu(conv2d(x_2, W_conv2) + b_conv2)
    conv2_ce = -tf.reduce_sum(x_2*tf.log(tf.clip_by_value(h_conv2,1e-10,100.0)))
    conv2_ts = tf.train.AdamOptimizer(1e-4).minimize(conv2_ce)
    h_pool2 = max_pool_2x2(h_conv2)#image:16*16, shape=[None, 16, 16, 32]

    x_3 = tf.placeholder("float", shape=[None, 16, 16, 32])
    W_conv3 = weight_variable([3, 3, 32, 64])
    b_conv3 = bias_variable([64])
    W3_hist = tf.histogram_summary("conv3_weights",W_conv3)
    b3_hist = tf.histogram_summary("conv3_biases",b_conv3)
    h_conv3 = tf.nn.relu(conv2d(x_3, W_conv3) + b_conv3)
    fake_W_conv3 = weight_variable([3, 3, 64, 32])
    fake_b_conv3 = bias_variable([32])
    fake_h_conv3 = tf.nn.relu(conv2d(h_conv3, fake_W_conv3) + fake_b_conv3)
    conv3_ce = -tf.reduce_sum(x_3*tf.log(tf.clip_by_value(fake_h_conv3,1e-10,100.0)))
    conv3_ts = tf.train.AdamOptimizer(1e-4).minimize(conv3_ce)

    x_4 = tf.placeholder("float", shape=[None, 16, 16, 64])
    W_conv4 = weight_variable([3, 3, 64, 64])
    b_conv4 = bias_variable([64])
    W4_hist = tf.histogram_summary("conv4_weights",W_conv4)
    b4_hist = tf.histogram_summary("conv4_biases",b_conv4)
    h_conv4 = tf.nn.relu(conv2d(x_4, W_conv4) + b_conv4)
    conv4_ce = -tf.reduce_sum(x_4*tf.log(tf.clip_by_value(h_conv4,1e-10,100.0)))
    conv4_ts = tf.train.AdamOptimizer(1e-4).minimize(conv4_ce)
    h_pool4 = max_pool_2x2(h_conv4)#image:8*8

    x_5 = tf.placeholder("float", shape=[None, 8, 8, 64])
    W_conv5 = weight_variable([3, 3, 64, 128])
    b_conv5 = bias_variable([128])
    W5_hist = tf.histogram_summary("conv5_weights",W_conv5)
    b5_hist = tf.histogram_summary("conv5_biases",b_conv5)
    h_conv5 = tf.nn.relu(conv2d(x_5, W_conv5) + b_conv5)
    fake_W_conv5 = weight_variable([3, 3, 128, 64])
    fake_b_conv5 = bias_variable([64])
    fake_h_conv5 = tf.nn.relu(conv2d(h_conv5, fake_W_conv5) + fake_b_conv5)
    conv5_ce = -tf.reduce_sum(x_5*tf.log(tf.clip_by_value(fake_h_conv5,1e-10,100.0)))
    conv5_ts = tf.train.AdamOptimizer(1e-4).minimize(conv5_ce)

    x_6 = tf.placeholder("float", shape=[None, 8, 8, 128])
    W_conv6 = weight_variable([3, 3, 128, 128])
    b_conv6 = bias_variable([128])
    W6_hist = tf.histogram_summary("conv6_weights",W_conv6)
    b6_hist = tf.histogram_summary("conv6_biases",b_conv6)
    h_conv6 = tf.nn.relu(conv2d(x_6, W_conv6) + b_conv6)
    conv6_ce = -tf.reduce_sum(x_6*tf.log(tf.clip_by_value(h_conv6,1e-10,100.0)))
    conv6_ts = tf.train.AdamOptimizer(1e-4).minimize(conv6_ce)

    x_7 = tf.placeholder("float", shape=[None, 8, 8, 128])
    W_conv7 = weight_variable([3, 3, 128, 128])
    b_conv7 = bias_variable([128])
    W7_hist = tf.histogram_summary("conv7_weights",W_conv7)
    b7_hist = tf.histogram_summary("conv7_biases",b_conv7)
    h_conv7 = tf.nn.relu(conv2d(x_7, W_conv7) + b_conv7)
    conv7_ce = -tf.reduce_sum(x_7*tf.log(tf.clip_by_value(h_conv7,1e-10,100.0)))
    conv7_ts = tf.train.AdamOptimizer(1e-4).minimize(conv7_ce)

    x_8 = tf.placeholder("float", shape=[None, 8, 8, 128])
    W_conv8 = weight_variable([3, 3, 128, 128])
    b_conv8 = bias_variable([128])
    W8_hist = tf.histogram_summary("conv8_weights",W_conv8)
    b8_hist = tf.histogram_summary("conv8_biases",b_conv8)
    h_conv8 = tf.nn.relu(conv2d(x_8, W_conv8) + b_conv8)
    conv8_ce = -tf.reduce_sum(x_8*tf.log(tf.clip_by_value(h_conv8,1e-10,100.0)))
    conv8_ts = tf.train.AdamOptimizer(1e-4).minimize(conv8_ce)
    h_pool8 = max_pool_2x2(h_conv8)#image:4*4

    x_9 = tf.placeholder("float", shape=[None, 4, 4, 128])
    W_conv9 = weight_variable([3, 3, 128, 256])
    b_conv9 = bias_variable([256])
    W9_hist = tf.histogram_summary("conv9_weights",W_conv9)
    b9_hist = tf.histogram_summary("conv9_biases",b_conv9)
    h_conv9 = tf.nn.relu(conv2d(x_9, W_conv9) + b_conv9)
    fake_W_conv9 = weight_variable([3, 3, 256, 128])
    fake_b_conv9 = bias_variable([128])
    fake_h_conv9 = tf.nn.relu(conv2d(h_conv9, fake_W_conv9) + fake_b_conv9)
    conv9_ce = -tf.reduce_sum(x_9*tf.log(tf.clip_by_value(fake_h_conv9,1e-10,100.0)))
    conv9_ts = tf.train.AdamOptimizer(1e-4).minimize(conv9_ce)

    x_10 = tf.placeholder("float", shape=[None, 4, 4, 256])
    W_conv10 = weight_variable([3, 3, 256, 256])
    b_conv10 = bias_variable([256])
    W10_hist = tf.histogram_summary("conv10_weights",W_conv10)
    b10_hist = tf.histogram_summary("conv10_biases",b_conv10)
    h_conv10 = tf.nn.relu(conv2d(x_10, W_conv10) + b_conv10)
    conv10_ce = -tf.reduce_sum(x_10*tf.log(tf.clip_by_value(h_conv10,1e-10,100.0)))
    conv10_ts = tf.train.AdamOptimizer(1e-4).minimize(conv10_ce)

    x_11 = tf.placeholder("float", shape=[None, 4, 4, 256])
    W_conv11 = weight_variable([3, 3, 256, 256])
    b_conv11 = bias_variable([256])
    W11_hist = tf.histogram_summary("conv11_weights",W_conv11)
    b11_hist = tf.histogram_summary("conv11_biases",b_conv11)
    h_conv11 = tf.nn.relu(conv2d(x_11, W_conv11) + b_conv11)
    conv11_ce = -tf.reduce_sum(x_11*tf.log(tf.clip_by_value(h_conv11,1e-10,100.0)))
    conv11_ts = tf.train.AdamOptimizer(1e-4).minimize(conv11_ce)

    x_12 = tf.placeholder("float", shape=[None, 4, 4, 256])
    W_conv12 = weight_variable([3, 3, 256, 256])
    b_conv12 = bias_variable([256])
    W12_hist = tf.histogram_summary("conv12_weights",W_conv12)
    b12_hist = tf.histogram_summary("conv12_biases",b_conv12)
    h_conv12 = tf.nn.relu(conv2d(x_12, W_conv12) + b_conv12)
    conv12_ce = -tf.reduce_sum(x_12*tf.log(tf.clip_by_value(h_conv12,1e-10,100.0)))
    conv12_ts = tf.train.AdamOptimizer(1e-4).minimize(conv12_ce)
    h_pool12 = max_pool_2x2(h_conv12)#image:2*2

    x_13 = tf.placeholder("float", shape=[None, 2, 2, 256])
    W_fc13 = weight_variable([2*2*256, 160])
    b_fc13 = bias_variable([160])
    W13_hist = tf.histogram_summary("fc13_weights",W_fc13)
    b13_hist = tf.histogram_summary("fc13_biases",b_fc13)
    h_pool12_flat = tf.reshape(x_13, [-1, 2*2*256])
    h_fc13 = tf.nn.relu(tf.matmul(h_pool12_flat, W_fc13) + b_fc13)
    h_fc13_drop = tf.nn.dropout(h_fc13, keep_prob)
    fake_W_fc13 = weight_variable([160, 2*2*256])
    fake_b_fc13 = bias_variable([2*2*256])
    fake_h_fc13 = tf.nn.relu(tf.matmul(h_fc13_drop, fake_W_fc13) + fake_b_fc13)
    fc13_ce = -tf.reduce_sum(h_pool12_flat*tf.log(tf.clip_by_value(fake_h_fc13,1e-10,100.0)))
    fc13_ts = tf.train.AdamOptimizer(1e-4).minimize(fc13_ce)

    x_14 = tf.placeholder("float", shape=[None, 160])
    W_fc14 = weight_variable([160, 160])
    b_fc14 = bias_variable([160])
    W14_hist = tf.histogram_summary("fc14_weights",W_fc14)
    b14_hist = tf.histogram_summary("fc14_biases",b_fc14)
    h_fc14 = tf.nn.relu(tf.matmul(x_14, W_fc14) + b_fc14)
    h_fc14_drop = tf.nn.dropout(h_fc14, keep_prob)
    fake_W_fc14 = weight_variable([160, 160])
    fake_b_fc14 = bias_variable([160])
    fake_h_fc14 = tf.nn.relu(tf.matmul(h_fc14_drop, fake_W_fc14) + fake_b_fc14)
    fc14_ce = -tf.reduce_sum(x_14*tf.log(tf.clip_by_value(fake_h_fc14,1e-10,100.0)))
    fc14_ts = tf.train.AdamOptimizer(1e-4).minimize(fc14_ce)

    x_15 = tf.placeholder("float", shape=[None, 160])
    W_fc15 = weight_variable([160, 2])
    b_fc15 = bias_variable([2])
    W15_hist = tf.histogram_summary("fc15_weights",W_fc15)
    b15_hist = tf.histogram_summary("fc15_biases",b_fc15)
    y_conv=tf.nn.softmax(tf.matmul(x_15, W_fc15) + b_fc15)

    cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,100.0)))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

    merged = tf.merge_all_summaries()
    sess.run(tf.initialize_all_variables())
    writer = tf.train.SummaryWriter("./tmp/wave_logs20150114",sess.graph_def)

    resultFile = open("result_deep20150114.csv","ab")
    csvwriter = csv.writer(resultFile)

    print "start training"
    for i in range(1001):
      if i%10 == 0:
          print "step %d"%i
          try:
              result = sess.run([merged])
              writer.add_summary(result[0],i)
          except Exception as e:
              print '=== エラー内容 ==='
              print 'type:' + str(type(e))
              print 'args:' + str(e.args)
              print 'message:' + e.message
              print 'e自身:' + str(e)
      batch = mnist.train.next_batch(30)
      conv1_ts.run(feed_dict={x: batch[0]})
      x2_in = sess.run(h_conv1, feed_dict={x: batch[0]})
      conv2_ts.run(feed_dict={x_2: x2_in})
      x3_in = sess.run(h_pool2, feed_dict={x_2: x2_in})
      conv3_ts.run(feed_dict={x_3: x3_in})
      x4_in = sess.run(h_conv3, feed_dict={x_3: x3_in})
      conv4_ts.run(feed_dict={x_4: x4_in})
      x5_in = sess.run(h_pool4, feed_dict={x_4: x4_in})
      conv5_ts.run(feed_dict={x_5: x5_in})
      x6_in = sess.run(h_conv5, feed_dict={x_5: x5_in})
      conv6_ts.run(feed_dict={x_6: x6_in})
      x7_in = sess.run(h_conv6, feed_dict={x_6: x6_in})
      conv7_ts.run(feed_dict={x_7: x7_in})
      x8_in = sess.run(h_conv7, feed_dict={x_7: x7_in})
      conv8_ts.run(feed_dict={x_8: x8_in})
      x9_in = sess.run(h_pool8, feed_dict={x_8: x8_in})
      conv9_ts.run(feed_dict={x_9: x9_in})
      x10_in = sess.run(h_conv9, feed_dict={x_9: x9_in})
      conv10_ts.run(feed_dict={x_10: x10_in})
      x11_in = sess.run(h_conv10, feed_dict={x_10: x10_in})
      conv11_ts.run(feed_dict={x_11: x11_in})
      x12_in = sess.run(h_conv11, feed_dict={x_11: x11_in})
      conv12_ts.run(feed_dict={x_12: x12_in})
      x13_in = sess.run(h_pool12, feed_dict={x_12: x12_in})
      fc13_ts.run(feed_dict={x_13: x13_in, keep_prob: 0.5})
      x14_in = sess.run(h_fc13_drop, feed_dict={x_13: x13_in, keep_prob: 0.5})
      fc14_ts.run(feed_dict={x_14: x14_in, keep_prob: 0.5})
      x15_in = sess.run(h_fc14_drop, feed_dict={x_14: x14_in, keep_prob: 0.5})
      if i%500 == 0:
        train_accuracy = accuracy.eval(feed_dict={x_15: x15_in, y_: batch[1], keep_prob: 1.0})
        x2_in_test = sess.run(h_conv1, feed_dict={x: mnist.test.images})
        x3_in_test = sess.run(h_pool2, feed_dict={x_2: x2_in_test})
        x4_in_test = sess.run(h_conv3, feed_dict={x_3: x3_in_test})
        x5_in_test = sess.run(h_pool4, feed_dict={x_4: x4_in_test})
        x6_in_test = sess.run(h_conv5, feed_dict={x_5: x5_in_test})
        x7_in_test = sess.run(h_conv6, feed_dict={x_6: x6_in_test})
        x8_in_test = sess.run(h_conv7, feed_dict={x_7: x7_in_test})
        x9_in_test = sess.run(h_pool8, feed_dict={x_8: x8_in_test})
        x10_in_test = sess.run(h_conv9, feed_dict={x_9: x9_in_test})
        x11_in_test = sess.run(h_conv10, feed_dict={x_10: x10_in_test})
        x12_in_test = sess.run(h_conv11, feed_dict={x_11: x11_in_test})
        x13_in_test = sess.run(h_pool12, feed_dict={x_12: x12_in_test})
        x14_in_test = sess.run(h_fc13_drop, feed_dict={x_13: x13_in_test, keep_prob: 1.0})
        x15_in_test = sess.run(h_fc14_drop, feed_dict={x_14: x14_in_test, keep_prob: 1.0})
        test_accuracy = accuracy.eval(feed_dict={x_15: x15_in_test, y_: mnist.test.labels, keep_prob: 1.0})
        csvwriter.writerow([i,train_accuracy,test_accuracy])
        print "step %d, training accuracy %g, test accuracy %g"%(i, train_accuracy, test_accuracy)
      elif i%50 == 0:
        train_accuracy = accuracy.eval(feed_dict={x_15: x15_in, y_: batch[1], keep_prob: 1.0})
        csvwriter.writerow([i,train_accuracy])
        print "step %d, training accuracy %g"%(i, train_accuracy)
      train_step.run(feed_dict={x_15: x15_in, y_: batch[1], keep_prob: 0.5})
    print "finish training"
    x2_in_test = sess.run(h_conv1, feed_dict={x: mnist.test.images})
    x3_in_test = sess.run(h_pool2, feed_dict={x_2: x2_in_test})
    x4_in_test = sess.run(h_conv3, feed_dict={x_3: x3_in_test})
    x5_in_test = sess.run(h_pool4, feed_dict={x_4: x4_in_test})
    x6_in_test = sess.run(h_conv5, feed_dict={x_5: x5_in_test})
    x7_in_test = sess.run(h_conv6, feed_dict={x_6: x6_in_test})
    x8_in_test = sess.run(h_conv7, feed_dict={x_7: x7_in_test})
    x9_in_test = sess.run(h_pool8, feed_dict={x_8: x8_in_test})
    x10_in_test = sess.run(h_conv9, feed_dict={x_9: x9_in_test})
    x11_in_test = sess.run(h_conv10, feed_dict={x_10: x10_in_test})
    x12_in_test = sess.run(h_conv11, feed_dict={x_11: x11_in_test})
    x13_in_test = sess.run(h_pool12, feed_dict={x_12: x12_in_test})
    x14_in_test = sess.run(h_fc13_drop, feed_dict={x_13: x13_in_test, keep_prob: 1.0})
    x15_in_test = sess.run(h_fc14_drop, feed_dict={x_14: x14_in_test, keep_prob: 1.0})
    print "test accuracy %g"%accuracy.eval(feed_dict={x_15: x15_in_test, y_: mnist.test.labels, keep_prob: 1.0})
    resultFile.close()


if __name__ == '__main__':
    conv_neural_network()
